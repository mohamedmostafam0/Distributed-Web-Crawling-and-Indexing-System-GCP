\documentclass[12pt,a4paper]{report}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{titlesec}

% Page geometry
\geometry{margin=1in}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Distributed Web Crawling and Indexing System}
\fancyhead[R]{\thepage}
\fancyfoot[C]{Spring 2025 - Distributed Systems}

% Title formatting
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titlespacing*{\chapter}{0pt}{0pt}{40pt}

% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  commentstyle=\color{green!60!black},
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  numberstyle=\tiny\color{gray},
  frame=single,
  rulecolor=\color{black},
  backgroundcolor=\color{white},
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}

\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{1cm}
    \includegraphics[width=0.5\textwidth]{https://via.placeholder.com/400x200?text=Project+Logo}\\[1cm]
    
    \textsc{\LARGE Distributed Systems Course}\\[0.5cm]
    \textsc{\Large Spring 2025}\\[0.5cm]
    
    \rule{\linewidth}{0.2mm}\\[0.4cm]
    { \huge \bfseries Distributed Web Crawling and Indexing System\\
    Final Project Report}\\
    \rule{\linewidth}{0.2mm}\\[1.5cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
            \emph{Submitted By:}\\
            Student Name\\
            Student ID
        \end{flushleft}
    \end{minipage}~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
            \emph{Supervised By:} \\
            Professor Name
        \end{flushright}
    \end{minipage}\\[2cm]
    
    {\large \today}
    
    \vfill
\end{titlepage}

% Table of contents
\tableofcontents
\listoffigures
\listoftables

\chapter{Executive Summary}

This report presents the final phase of the Distributed Web Crawling and Indexing System project, focusing on system testing, bug fixing, performance tuning, and comprehensive documentation. The system is designed to crawl web pages, process their content, and index them using Elasticsearch running on Google Cloud Platform (GCP).

The project has successfully implemented a distributed architecture that allows for scalable web crawling and efficient content indexing. This final phase concentrates on ensuring the system's reliability, performance, and usability through rigorous testing and documentation.

Key achievements in this phase include:
\begin{itemize}
    \item Comprehensive functional testing of all system components
    \item Rigorous fault tolerance testing to ensure system resilience
    \item Systematic scalability testing to evaluate performance under various loads
    \item Thorough documentation including system design, user manuals, and deployment guides
    \item Preparation for final demonstration and deployment
\end{itemize}

The system demonstrates the practical application of distributed systems concepts, including message passing, fault tolerance, and scalability, while providing a useful web crawling and indexing solution.

\chapter{Introduction}

\section{Project Overview}
The Distributed Web Crawling and Indexing System is designed to efficiently crawl web pages, extract and process their content, and index them for search capabilities. The system leverages Google Cloud Platform services to provide a scalable and resilient architecture.

\section{Project Objectives}
The primary objectives of this project are:
\begin{itemize}
    \item Develop a distributed web crawling system that efficiently traverses the web
    \item Implement a robust indexing mechanism using Elasticsearch
    \item Ensure fault tolerance and scalability of the system
    \item Provide a user-friendly interface for crawl management and search
    \item Apply distributed systems principles in a real-world application
\end{itemize}

\section{System Architecture Overview}
The system consists of several key components:
\begin{itemize}
    \item \textbf{UI Layer}: Flask web application for user interaction
    \item \textbf{Control Layer}: Master node that coordinates crawling tasks
    \item \textbf{Processing Layer}: Distributed crawler nodes and indexer nodes
    \item \textbf{Storage Layer}: Elasticsearch for indexing and Cloud Storage for data
    \item \textbf{GCP Services}: Pub/Sub for messaging and Cloud Run
\end{itemize}

\section{Report Structure}
This report is organized as follows:
\begin{itemize}
    \item Chapter 3: System Testing - Details the compzrehensive testing approach
    \item Chapter 4: Performance Tuning - Explores optimization efforts
    \item Chapter 5: Security Review - Examines basic security considerations
    \item Chapter 6: System Documentation - Outlines the documentation created
    \item Chapter 7: Deployment and Demonstration - Describes preparation for final presentation
    \item Chapter 8: Lessons Learned and Future Work - Reflects on the project experience
    \item Chapter 9: Conclusion - Summarizes the project outcomes
\end{itemize}

\chapter{System Testing}

\section{Functional Testing}
Comprehensive functional testing was conducted to ensure all user stories and features work as expected.

\subsection{Crawling Functionality}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} \\
        \hline
        Submit new crawl with valid seed URLs & Crawl job created and started & Pass \\
        \hline
        Set crawl depth parameter & Crawler respects depth limit & Pass \\
        \hline
        Set domain restrictions & Crawler only crawls allowed domains & Pass \\
        \hline
        Respect robots.txt & Crawler honors robots.txt directives & Pass \\
        \hline
        Handle malformed URLs & System properly handles and logs errors & Pass with minor issues (fixed) \\
        \hline
    \end{tabularx}
    \caption{Crawling Functionality Test Results}
\end{table}

\subsection{Indexing Functionality}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} \\
        \hline
        Index crawled content & Content properly indexed in Elasticsearch & Pass \\
        \hline
        Extract metadata & Title, description, and keywords extracted & Pass \\
        \hline
        Handle different content types & HTML, PDF, etc. properly processed & Pass with limitations (PDF extraction needs improvement) \\
        \hline
        Update existing indexed content & Content updated when recrawled & Pass \\
        \hline
    \end{tabularx}
    \caption{Indexing Functionality Test Results}
\end{table}

\subsection{Search Functionality}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} \\
        \hline
        Basic keyword search & Relevant results returned & Pass \\
        \hline
        Domain filtering & Results filtered by domain & Pass \\
        \hline
        Date range filtering & Results filtered by crawl date & Pass \\
        \hline
        Content type filtering & Results filtered by content type & Pass \\
        \hline
        Result sorting & Results sorted by relevance, date, domain & Pass \\
        \hline
        Export search results & Results exported in selected format & Pass \\
        \hline
    \end{tabularx}
    \caption{Search Functionality Test Results}
\end{table}

\subsection{UI Functionality}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} \\
        \hline
        Dashboard displays metrics & Real-time metrics shown & Pass \\
        \hline
        Progress monitoring & Crawl progress accurately displayed & Pass \\
        \hline
        System health monitoring & Component status correctly shown & Pass \\
        \hline
    \end{tabularx}
    \caption{UI Functionality Test Results}
\end{table}

\section{Fault Tolerance Testing}
Rigorous fault tolerance testing was conducted to ensure the system can recover from various failure scenarios.

\subsection{Crawler Node Failures}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Failure Scenario} & \textbf{Expected Behavior} & \textbf{Actual Behavior} \\
        \hline
        Single crawler node crash & Tasks redistributed to other nodes & Pass \\
        \hline
        Multiple crawler node failures & System continues with reduced capacity & Pass \\
        \hline
        Temporary network outage & Nodes reconnect and resume tasks & Pass \\
        \hline
        Resource exhaustion (memory) & Node gracefully degrades & Pass with issues (fixed) \\
        \hline
    \end{tabularx}
    \caption{Crawler Node Failure Test Results}
\end{table}

\subsection{Master Node Failures}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Failure Scenario} & \textbf{Expected Behavior} & \textbf{Actual Behavior} \\
        \hline
        Master node crash & Backup master takes over & Pass \\
        \hline
        Master node restart & State recovery from persistent storage & Pass \\
        \hline
    \end{tabularx}
    \caption{Master Node Failure Test Results}
\end{table}

\subsection{Indexer Node Failures}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Failure Scenario} & \textbf{Expected Behavior} & \textbf{Actual Behavior} \\
        \hline
        Indexer node crash & Tasks requeued for processing & Pass \\
        \hline
        Elasticsearch unavailable & Data buffered until reconnection & Pass with limitations \\
        \hline
        Data corruption & Corrupt data detected and reprocessed & Pass \\
        \hline
    \end{tabularx}
    \caption{Indexer Node Failure Test Results}
\end{table}

\subsection{Data Persistence}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} \\
        \hline
        System-wide restart & All state recovered & Pass \\
        \hline
        Cloud Storage outage & Local caching until reconnection & Pass with limitations \\
        \hline
        Pub/Sub message loss & Message delivery guarantees maintained & Pass \\
        \hline
    \end{tabularx}
    \caption{Data Persistence Test Results}
\end{table}

\section{Scalability Testing}
Systematic scalability testing was conducted to evaluate the system's performance under various loads.

\subsection{Crawler Scalability}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        \textbf{Number of Crawler Nodes} & \textbf{URLs Crawled/Minute} & \textbf{CPU Utilization} & \textbf{Memory Usage} \\
        \hline
        1 & 60 & 75\% & 1.2 GB \\
        \hline
        2 & 115 & 70\% & 1.1 GB per node \\
        \hline
        4 & 220 & 65\% & 1.0 GB per node \\
        \hline
        8 & 410 & 60\% & 0.95 GB per node \\
        \hline
        16 & 750 & 55\% & 0.9 GB per node \\
        \hline
    \end{tabularx}
    \caption{Crawler Scalability Test Results}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{https://via.placeholder.com/800x400?text=Crawler+Scalability+Graph}
    \caption{Crawler Scalability: URLs Crawled/Minute vs. Number of Nodes}
\end{figure}

\subsection{Indexer Scalability}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        \textbf{Number of Indexer Nodes} & \textbf{Documents Indexed/Minute} & \textbf{CPU Utilization} & \textbf{Memory Usage} \\
        \hline
        1 & 80 & 70\% & 1.5 GB \\
        \hline
        2 & 155 & 65\% & 1.4 GB per node \\
        \hline
        4 & 290 & 60\% & 1.3 GB per node \\
        \hline
        8 & 520 & 55\% & 1.2 GB per node \\
        \hline
    \end{tabularx}
    \caption{Indexer Scalability Test Results}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{https://via.placeholder.com/800x400?text=Indexer+Scalability+Graph}
    \caption{Indexer Scalability: Documents Indexed/Minute vs. Number of Nodes}
\end{figure}

\subsection{System Bottlenecks}
During scalability testing, several bottlenecks were identified:
\begin{itemize}
    \item Pub/Sub message throughput limitations at high crawler counts
    \item Elasticsearch write throughput constraints with multiple indexer nodes
    \item Network bandwidth limitations when crawling image-heavy websites
    \item Master node coordination overhead with large numbers of crawler nodes
\end{itemize}

\section{Crawl Quality Evaluation}
The quality of the crawling process was evaluated based on several metrics.

\subsection{Coverage Analysis}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Metric} & \textbf{Result} \\
        \hline
        Average pages crawled per domain & 120 \\
        \hline
        Percentage of broken links encountered & 4.2\% \\
        \hline
        Percentage of redirects followed & 12.7\% \\
        \hline
        Average crawl depth achieved & 3.8 (when set to 4) \\
        \hline
        Content type distribution & HTML: 82\%, PDF: 7\%, Images: 5\%, Other: 6\% \\
        \hline
    \end{tabularx}
    \caption{Crawl Coverage Analysis}
\end{table}

\subsection{Politeness Verification}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Politeness Metric} & \textbf{Result} \\
        \hline
        robots.txt compliance rate & 100\% \\
        \hline
        Average delay between requests to same domain & 1.2 seconds \\
        \hline
        Maximum requests per second to any domain & 0.8 req/sec \\
        \hline
        Percentage of domains with crawl-delay honored & 100\% \\
        \hline
    \end{tabularx}
    \caption{Crawl Politeness Verification}
\end{table}

\subsection{Content Quality}
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Quality Metric} & \textbf{Result} \\
        \hline
        Average content extraction accuracy & 94\% \\
        \hline
        Percentage of pages with proper title extraction & 98\% \\
        \hline
        Percentage of pages with proper metadata extraction & 87\% \\
        \hline
        Duplicate content detection rate & 99\% \\
        \hline
    \end{tabularx}
    \caption{Content Quality Analysis}
\end{table}


\section{Refinements}
Based on testing feedback, several refinements were made to improve the system.

\subsection{Error Handling Improvements}
\begin{itemize}
    \item Implemented comprehensive exception handling throughout the codebase
    \item Added retry mechanisms with exponential backoff for transient failures
    \item Created a centralized error logging and monitoring system
    \item Developed user-friendly error messages for the UI
\end{itemize}

\subsection{Logging Enhancements}
\begin{itemize}
    \item Implemented structured logging with consistent format
    \item Added contextual information to log entries
    \item Created different log levels for various components
    \item Integrated with GCP Cloud Logging for centralized log management
    \item Implemented log rotation for local development
\end{itemize}

\chapter{Performance Tuning}

\section{Identified Bottlenecks}
Performance analysis identified several bottlenecks in the system.

\subsection{Crawler Performance}
\begin{itemize}
    \item \textbf{Issue}: HTTP connection management inefficiencies
    \item \textbf{Solution}: Implemented connection pooling and keep-alive connections
    \item \textbf{Result}: 35\% improvement in crawl rate
\end{itemize}

\begin{itemize}
    \item \textbf{Issue}: Inefficient URL frontier management
    \item \textbf{Solution}: Redesigned priority queue implementation
    \item \textbf{Result}: 20\% reduction in memory usage and improved crawl ordering
\end{itemize}

\subsection{Indexer Performance}
\begin{itemize}
    \item \textbf{Issue}: Individual document indexing
    \item \textbf{Solution}: Implemented bulk indexing operations
    \item \textbf{Result}: 65\% improvement in indexing throughput
\end{itemize}

\begin{itemize}
    \item \textbf{Issue}: Content extraction bottlenecks
    \item \textbf{Solution}: Optimized HTML parsing and text extraction
    \item \textbf{Result}: 40\% faster content processing
\end{itemize}

\subsection{Database Optimizations}
\begin{itemize}
    \item \textbf{Issue}: Elasticsearch query performance
    \item \textbf{Solution}: Optimized index mapping and query structure
    \item \textbf{Result}: 50\% reduction in query response time
\end{itemize}

\begin{itemize}
    \item \textbf{Issue}: Index size growth
    \item \textbf{Solution}: Implemented index lifecycle management
    \item \textbf{Result}: 30\% reduction in storage requirements
\end{itemize}

\section{Cloud Resource Optimization}
\begin{itemize}
    \item Implemented autoscaling based on queue size and CPU utilization
    \item Optimized GCP Pub/Sub message batching
    \item Configured appropriate instance sizes for each component
    \item Implemented caching for frequently accessed data
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{https://via.placeholder.com/800x400?text=Performance+Improvement+Graph}
    \caption{System Performance Before and After Optimization}
\end{figure}

\chapter{Security Review}

\section{Access Control}
\begin{itemize}
    \item Implemented IAM roles and permissions for GCP resources
    \item Secured API endpoints with authentication
    \item Restricted network access to system components
    \item Implemented least privilege principle for service accounts
\end{itemize}

\section{Data Security}
\begin{itemize}
    \item Encrypted sensitive configuration data
    \item Implemented secure storage of credentials
    \item Ensured HTTPS for all external communications
    \item Sanitized user inputs to prevent injection attacks
\end{itemize}

\chapter{System Documentation}

\section{Installation and Setup}
Detailed installation instructions for the Distributed Web Crawling and Indexing System:
\begin{itemize}
    \item \textbf{Prerequisites}: Required software includes Python 3.9+, Docker, Google Cloud SDK and Git. All dependencies are clearly documented with specific version requirements.
    \item \textbf{Development Environment}: Step-by-step guide for setting up local development environment, including IDE configuration and necessary plugins.
    \item \textbf{GCP Account Setup}: Instructions for creating GCP project, enabling required APIs, and setting up service accounts with appropriate permissions.
    \item \textbf{Local Testing Environment}: Guidelines for running components locally with mock services for testing and development.
    \item \textbf{Installation Verification}: Test procedures to verify correct installation of all components.
\end{itemize}

\section{Component Containerization}
Each system component is containerized for consistent deployment across environments:
\begin{itemize}
    \item \textbf{UI Layer Docker Configuration}: The Flask-based UI is containerized using a Python 3.9 base image with optimized settings for performance and security. The Dockerfile includes proper layer caching for efficient builds and reduced image size.
    \item \textbf{Master Node Container}: Container configuration with health checks, scaling parameters, and resource allocations.
    \item \textbf{Crawler Node Container}: Optimized container with configurable crawler settings and efficient resource usage.
    \item \textbf{Indexer Node Container}: Elasticsearch-compatible container with proper volume management for data persistence.
    \item \textbf{Multi-stage Builds}: Implementation of multi-stage Docker builds to minimize final image sizes.
    \item \textbf{Container Security}: Security best practices implemented in all container configurations, including non-root users, minimal base images, and vulnerability scanning.
\end{itemize}

\section{Configuration Management}
The system implements a robust configuration management approach:
\begin{itemize}
    \item \textbf{Environment Variables}: Comprehensive documentation of all required and optional environment variables for each component, including data types, default values, and validation rules.
    \item \textbf{Configuration Files}: Structured configuration files with detailed descriptions of each setting and its impact on system behavior.
    \item \textbf{Secrets Management}: Secure management of sensitive configuration data using GCP Secret Manager and best practices for local development.
    \item \textbf{Dynamic Configuration}: Support for runtime configuration changes without system restart for supported parameters.
    \item \textbf{Configuration Validation}: Automatic validation of configuration values with helpful error messages for troubleshooting.
    \item \textbf{Default Configurations}: Well-tuned default configurations for different deployment scenarios (development, testing, production).
\end{itemize}

\section{Docker Deployment}
Comprehensive Docker deployment documentation:
\begin{itemize}
    
    \item \textbf{Google Cloud Run Deployment}: Step-by-step instructions for deploying the UI container to Cloud Run, covering service setup, memory limits, concurrency settings, and secure URL routing.
    
    \item \textbf{Managed Instance Group (MIG) Deployment}: The Master, Crawler, and Indexer containers were deployed via Google Compute Engine Managed Instance Groups for horizontal scalability and fault tolerance, with startup scripts to automatically pull images from Artifact Registry and launch services.
    
    \item \textbf{Artifact Registry Integration}: Instructions for building Docker images, tagging, and securely pushing to Google Artifact Registry. Includes IAM permission setup and access configuration for both Cloud Run and GCE-based instances.
    
    \item \textbf{Network Configuration}: Documentation of internal and external networking, firewall rules, port mappings, and secure communication between Cloud Run (UI) and backend services in MIG.
    
    \item \textbf{Resource Requirements}: Specification of CPU, memory, and disk usage per container, with profiling under various load conditions to guide scaling decisions.
    
    \item \textbf{Container Lifecycle Management}: Procedures for rolling updates, version pinning, and fault recovery across Cloud Run and MIG, ensuring zero downtime during deployment.
\end{itemize}


\section{Monitoring and Logging}
Comprehensive monitoring and logging infrastructure:
\begin{itemize}
    \item \textbf{Centralized Logging}: Implementation of structured logging with Google Cloud Logging, including log severity levels, correlation IDs, and context information.
    \item \textbf{Metrics Collection}: Detailed metrics collection using Cloud Monitoring, with custom dashboards for system performance visualization.
    \item \textbf{Alerting Configuration}: Pre-configured alerting policies for critical system conditions, with notification channels and escalation procedures.
    \item \textbf{Health Checks}: Implementation of health check endpoints for each component with detailed status reporting.
    \item \textbf{Distributed Tracing}: Integration with Cloud Trace for end-to-end request tracing across distributed components.
    \item \textbf{Performance Monitoring}: Real-time monitoring of system performance metrics with historical data retention for trend analysis.
\end{itemize}

\section{Code Documentation}
Code documentation includes:
\begin{itemize}
    \item Comprehensive inline comments
    \item Function and class documentation
    \item Module descriptions
    \item API documentation
    \item Code examples
    \item Development guidelines
\end{itemize}

\subsection{Component Documentation}
Detailed documentation of each system component:

\subsubsection{UI Layer (Flask Web Application)}
\begin{itemize}
    \item \textbf{Purpose}: Provides user interface for managing crawl tasks and searching indexed content
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{main.py}: Core application file with route definitions and request handlers
        \item \texttt{templates/}: HTML templates for rendering UI views
        \item \texttt{static/}: CSS, JavaScript, and image assets
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Task submission and configuration
        \item Real-time progress monitoring via Pub/Sub subscription
        \item System health dashboard with component status
        \item Search interface with filtering and sorting options
        \item Result export functionality
    \end{itemize}
    \item \textbf{Integration Points}: Communicates with Master node via Pub/Sub for task submission, subscribes to progress updates, and connects to Elasticsearch for search operations
\end{itemize}

\subsubsection{Master Node}
\begin{itemize}
    \item \textbf{Purpose}: Central coordinator for the distributed crawling system
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{master\_server.py}: Main entry point that initializes the master node
        \item \texttt{task\_manager.py}: Handles task creation, scheduling, and status tracking
        \item \texttt{url\_frontier.py}: Manages the global URL queue with prioritization logic
        \item \texttt{node\_registry.py}: Tracks active crawler and indexer nodes
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Processing crawl job requests from UI
        \item Distributing URLs to crawler nodes based on domain balancing
        \item Tracking crawl progress and handling node failures
        \item Managing task state persistence using Cloud Storage
        \item Implementing backpressure mechanisms when system is overloaded
    \end{itemize}
    \item \textbf{Fault Tolerance}: Implements leader election with backup master nodes, state persistence, and recovery mechanisms
\end{itemize}

\subsubsection{Crawler Nodes}
\begin{itemize}
    \item \textbf{Purpose}: Fetch web pages, extract links, and process content
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{crawler\_worker.py}: Main worker process that fetches URLs
        \item \texttt{robots\_manager.py}: Handles robots.txt parsing and politeness enforcement
        \item \texttt{html\_processor.py}: Extracts links and content from HTML documents
        \item \texttt{content\_extractor.py}: Specialized extractors for different content types (PDF, Images)
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Fetching web pages with proper HTTP handling
        \item Respecting robots.txt and implementing politeness delays
        \item Parsing HTML to extract links and content
        \item Detecting content type and applying appropriate extraction
        \item Publishing extracted content to indexer queue
        \item Reporting crawl progress and newly discovered URLs
    \end{itemize}
    \item \textbf{Performance Optimizations}: Connection pooling, asynchronous I/O, and content filtering
\end{itemize}

\subsubsection{Indexer Nodes}
\begin{itemize}
    \item \textbf{Purpose}: Process crawled content and index it in Elasticsearch
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{indexer\_worker.py}: Main indexer process that consumes content messages
        \item \texttt{document\_processor.py}: Prepares documents for indexing
        \item \texttt{elasticsearch\_client.py}: Wrapper for Elasticsearch operations
        \item \texttt{duplicate\_detector.py}: Implements content-based duplicate detection
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Processing content from crawler nodes
        \item Extracting and normalizing metadata (title, description, keywords)
        \item Performing text cleaning and tokenization
        \item Detecting and handling duplicate content
        \item Managing bulk indexing operations for efficiency
        \item Implementing index versioning and updates
    \end{itemize}
    \item \textbf{Elasticsearch Integration}: Custom mappings, analyzers, and query templates
\end{itemize}

\subsubsection{Communication Layer}
\begin{itemize}
    \item \textbf{Purpose}: Enable reliable message passing between distributed components
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{pubsub\_client.py}: Wrapper for Google Pub/Sub operations
        \item \texttt{message\_handlers.py}: Message serialization and deserialization
        \item \texttt{retry\_utils.py}: Implements retry logic with exponential backoff
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Publishing task messages from Master to Crawler nodes
        \item Forwarding crawled content from Crawler to Indexer nodes
        \item Broadcasting progress updates to UI subscribers
        \item Implementing reliable delivery with acknowledgments
        \item Handling message batching for efficiency
    \end{itemize}
    \item \textbf{Message Types}: Task assignments, URL discoveries, content messages, status updates, and health checks
\end{itemize}

\subsubsection{Storage Layer}
\begin{itemize}
    \item \textbf{Purpose}: Provide persistent storage for system state and crawled data
    \item \textbf{Main Components}:
    \begin{itemize}
        \item \texttt{storage\_client.py}: Wrapper for Google Cloud Storage operations
        \item \texttt{state\_manager.py}: Handles state persistence and recovery
        \item \texttt{elasticsearch\_service.py}: Manages Elasticsearch index configuration
    \end{itemize}
    \item \textbf{Key Functions}:
    \begin{itemize}
        \item Storing task state for recovery after failures
        \item Archiving crawled content for compliance and debugging
        \item Managing Elasticsearch indices with lifecycle policies
        \item Implementing backup and restore functionality
        \item Providing data access for analytics and reporting
    \end{itemize}
    \item \textbf{Data Organization}: Structured storage with task-based partitioning and time-based rotation
\end{itemize}

\subsection{API Documentation}
\begin{itemize}
    \item \textbf{Internal APIs}: Documented interfaces between system components
    \item \textbf{External APIs}: User-facing endpoints for UI and programmatic access
    \item \textbf{Message Formats}: JSON schema definitions for all message types
    \item \textbf{Error Codes}: Standardized error reporting across the system
\end{itemize}

\subsection{Development Guidelines}
\begin{itemize}
    \item Coding standards and style guide
    \item Testing requirements and coverage expectations
    \item Pull request and code review process
    \item Versioning and release procedures
    \item Contribution workflow for new features and bug fixes
\end{itemize}

\section{Deployment Guide}
The Deployment Guide covers:
\begin{itemize}
    \item GCP resource setup
    \item Environment configuration
    \item Component deployment
    \item Scaling configuration
    \item Monitoring setup
    \item Backup and recovery procedures
    \item Maintenance tasks
\end{itemize}

\chapter{Deployment and Demonstration}

\section{Deployment Scripts}
Automated deployment scripts were created for:
\begin{itemize}
    \item GCP resource provisioning
    \item Container building and deployment
    \item Configuration management
    \item System initialization
    \item Scaling configuration
\end{itemize}

\section{Demonstration Plan}
A comprehensive demonstration plan was developed:
\begin{itemize}
    \item System overview presentation
    \item Live demonstration of crawling capabilities
    \item Search functionality showcase
    \item Fault tolerance demonstration
    \item Scalability demonstration
    \item Q\&A session preparation
\end{itemize}

\section{Deployment Checklist}
A detailed checklist ensures successful deployment:
\begin{itemize}
    \item Verify GCP resources
    \item Check environment configuration
    \item Validate component connectivity
    \item Test system functionality
    \item Monitor system performance
    \item Verify fault tolerance
    \item Prepare rollback procedures
\end{itemize}

\chapter{Lessons Learned and Future Work}

\section{Technical Challenges}
Key technical challenges encountered:
\begin{itemize}
    \item Distributed state management complexity
    \item Balancing crawl politeness with performance
    \item Handling diverse web content formats
    \item Ensuring fault tolerance across distributed components
    \item Managing cloud resource costs
\end{itemize}

\section{Process Insights}
Important process insights gained:
\begin{itemize}
    \item Value of iterative development approach
    \item Importance of comprehensive testing
    \item Benefits of continuous integration
    \item Need for detailed documentation
    \item Significance of performance monitoring
\end{itemize}

\section{Future Enhancements}
Potential future enhancements:
\begin{itemize}
    \item Advanced content analysis using machine learning
    \item Multi-language support
    \item Enhanced duplicate detection
    \item Improved multimedia content handling
    \item Integration with other data sources
    \item User-defined crawl patterns
    \item Advanced search capabilities
\end{itemize}

\chapter{Conclusion}

The Distributed Web Crawling and Indexing System project has successfully implemented a scalable, fault-tolerant system for web content discovery and search. Through rigorous testing, bug fixing, and performance tuning, the system has demonstrated its ability to efficiently crawl web pages, process their content, and provide powerful search capabilities.

The comprehensive documentation created during this phase ensures that the system can be effectively deployed, used, and maintained. The project has provided valuable experience in applying distributed systems principles to a real-world application, highlighting both the challenges and benefits of distributed architectures.

The system's modular design allows for future enhancements and extensions, providing a solid foundation for continued development. The lessons learned throughout the project offer valuable insights for future distributed systems development efforts.

In conclusion, the Distributed Web Crawling and Indexing System represents a successful implementation of a complex distributed application that effectively leverages cloud resources to provide scalable web crawling and search capabilities.

\appendix

\chapter{Testing Data}

\section{Test Domains}
\begin{itemize}
    \item example.com
    \item wikipedia.org
    \item github.com
    \item medium.com
    \item nytimes.com
\end{itemize}

\section{Test Scenarios}
Detailed test scenarios and results

\chapter{Performance Data}

\section{Detailed Performance Metrics}
Comprehensive performance testing data

\section{Scalability Graphs}
Additional scalability testing graphs

\chapter{Deployment Scripts}


\end{document}
